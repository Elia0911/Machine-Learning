# -*- coding: utf-8 -*-
"""Toxic Tweet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yGj7v4nzrlu_TjjrYn_OYgZkVOs8Shkv

## Setup
"""

# Getting the GitHub repository
! git clone https://github.com/doguilmak/Toxic-Tweet-Analysis

# Importing Libraries
import re # regular expression matching operations (Unicode strings & 8-bit strings)
import string 
import os
import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

"""## Read Data"""

# Reading the CSV file
# The CSV file contines Toxic & None Toxic tweets
df=pd.read_csv('/content/Toxic-Tweet-Analysis/FinalBalancedDataset.csv')
LABEL=df['Toxicity']
df

LABEL.value_counts()

df.info()

df.isnull().sum()

df.duplicated().sum()

plt.figure(figsize = (15, 12))
sns.set_style('whitegrid')    
sns.histplot(data=LABEL)
plt.title("Toxic Tweet Analysis on Histogram")
plt.xlabel("Toxic Types")
plt.ylabel("Reviews in Total")
plt.show()

labels=df.Toxicity
labels.head(10)

"""## Data Preprossing"""

def remove_emoji(text):
    emoji_pattern = re.compile("["
                u"\U0001F600-\U0001F64F" #emoticons
                u"\U0001F300-\U0001F5FF" #symbols & pictograms
                u"\U0001F680-\U0001F6FF" #transport & map symbols
                u"\U0001F1E0-\U0001F1FF" #flags(ios)
                u"\U00002702-\U000027B0"
                u"\U000024C2-\U0001F251" 
                "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def clean_text(text):
    delete_dict = {sp_character: '' for sp_character in string.punctuation}
    delete_dict[' '] = ' '
    table = str.maketrans(delete_dict)
    text1 = text.translate(table)
    textArr = text1.split()
    text2 = ' '.join([w for w in textArr if(not w.isdigit() and (not w.isdigit() and len(w) > 3))])

    return text2.lower()

df['tweet'] = df['tweet'].apply(remove_emoji)
df['tweet'] = df['tweet'].apply(clean_text)

df

"""## Training"""

# Split the data 
x_train, x_test, y_train, y_test=train_test_split(df['tweet'], labels, test_size=0.2, random_state=0)

# TF_IDF 
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7) # Try to change the stop_words list for something better
print (tfidf_vectorizer)

tfidf_train = tfidf_vectorizer.fit_transform(x_train) # Fit to data, then transform it.
tfidf_test = tfidf_vectorizer.transform(x_test) # Transform a count matrix to a normalized tf or tf-idf representation.

pac = PassiveAggressiveClassifier(max_iter=100, random_state=0,tol=1e-3)
pac.fit(tfidf_train, y_train)

y_pred = pac.predict(tfidf_test)
score = accuracy_score(y_test, y_pred)
print(f'Accuracy score: {score}')

cm=confusion_matrix(y_test, y_pred, labels=[0, 1])
print("\nConfusion Matrix (PassiveAggressiveClassifier):\n", cm)

"""## Testing """

class_map = {0: "Not Toxic", 
            1: "Toxic"}

# pred = ["@LlishaH LOL! It's an acronym for: that hoe over there."]
pred = ["@LlishaH dump guy"]
my_pred = tfidf_vectorizer.transform(pred)

my_predClass = pac.predict(my_pred)
print(f"Model predicted your sentiment as {class_map[int(my_predClass[0])]}.")

"""## Saving Model"""

import pickle

pickle_file = open('pac.pkl', 'ab')
pickle.dump(pac, pickle_file)                     
pickle_file.close()

